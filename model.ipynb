{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reconstruct_midi_file (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Statistics: mean\n",
    "using BSON\n",
    "include(\"preprocessing.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_data (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_data(input_dir::String)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for csv in readdir(input_dir)\n",
    "        df = CSV.read(joinpath(input_dir, csv), DataFrame)\n",
    "        if size(df)[1] < 500\n",
    "            continue\n",
    "        end\n",
    "        push!(inputs, Matrix{Float32}(df[1:500, 1:end-1]))\n",
    "        push!(outputs, sum(df[1:500, end]))\n",
    "    end\n",
    "    \n",
    "    # Find the length of the longest input array\n",
    "    max_length = maximum(size(input, 1) for input in inputs)\n",
    "\n",
    "    # Pad input arrays with zeros to match the longest array's length\n",
    "    padded_inputs = []\n",
    "    for input in inputs\n",
    "        rows_to_pad = max_length - size(input, 1)\n",
    "        padded_input = vcat(input, zeros(Float32, rows_to_pad, size(input, 2)))\n",
    "        push!(padded_inputs, padded_input)\n",
    "    end\n",
    "\n",
    "    (padded_inputs, outputs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Float32[68.0 40.0 5280.0 1110.0; 65.0 34.0 5052.0 480.0; … ; 54.0 38.0 126720.0 1440.0; 44.0 51.0 126555.0 360.0], Float32[56.0 65.0 1.0 480.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[43.0 27.0 1.0 1156.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[54.0 65.0 0.0 582.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[56.0 65.0 1.0 480.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[56.0 65.0 1.0 480.0; 60.0 77.0 0.0 1430.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[56.0 65.0 1.0 480.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[56.0 65.0 1.0 480.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[52.0 65.0 1.0 662.0; 68.0 77.0 1.0 480.0; … ; 75.0 45.0 73800.0 60.0; 70.0 45.0 73920.0 480.0], Float32[56.0 65.0 1.0 480.0; 68.0 97.0 1.0 1080.0; … ; 75.0 9.0 73998.0 60.0; 70.0 45.0 73920.0 480.0]  …  Float32[81.0 48.0 51840.0 80.0; 79.0 35.0 51920.0 80.0; … ; 77.0 27.0 476720.0 80.0; 75.0 31.0 476800.0 80.0], Float32[81.0 48.0 51783.0 80.0; 79.0 35.0 51920.0 80.0; … ; 77.0 27.0 476720.0 80.0; 84.0 31.0 476556.0 80.0], Float32[81.0 48.0 51840.0 80.0; 78.0 35.0 51920.0 387.0; … ; 77.0 27.0 476720.0 80.0; 61.0 31.0 476800.0 515.0], Float32[81.0 48.0 52096.0 80.0; 75.0 15.0 52269.0 80.0; … ; 81.0 27.0 476720.0 891.0; 75.0 65.0 476872.0 80.0], Float32[81.0 48.0 51840.0 80.0; 79.0 35.0 51920.0 80.0; … ; 77.0 27.0 476720.0 80.0; 75.0 31.0 476800.0 80.0], Float32[81.0 48.0 51840.0 80.0; 79.0 35.0 51920.0 80.0; … ; 77.0 63.0 476928.0 520.0; 66.0 31.0 476620.0 80.0], Float32[69.0 71.0 51840.0 80.0; 79.0 35.0 52106.0 80.0; … ; 73.0 27.0 477129.0 80.0; 75.0 61.0 477209.0 80.0], Float32[81.0 48.0 51840.0 80.0; 86.0 35.0 52265.0 80.0; … ; 83.0 27.0 476720.0 80.0; 75.0 31.0 476983.0 80.0], Float32[93.0 48.0 51840.0 80.0; 87.0 67.0 52202.0 603.0; … ; 79.0 60.0 476974.0 508.0; 80.0 56.0 476610.0 80.0], Float32[69.0 11.0 51840.0 605.0; 67.0 74.0 51571.0 891.0; … ; 91.0 27.0 476720.0 696.0; 75.0 31.0 477276.0 80.0]], Any[961, 44, 103, 146, 199, 251, 295, 345, 415, 472  …  505, 568, 600, 711, 679, 791, 833, 861, 920, 932])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets = prepare_data(\"assets/anomalous\")\n",
    "\n",
    "trainRange = 1:Int(floor(0.8 * length(inputs)))\n",
    "valRange = Int(floor(0.8 * length(inputs))):length(inputs)\n",
    "\n",
    "train_data = inputs[trainRange], targets[trainRange]\n",
    "val_data = inputs[valRange], targets[valRange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSVs: 8550 Min Rows: 500 Max Rows: 500 Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# inputs, outputs = prepare_data(\"assets/anomalous\")\n",
    "\n",
    "rows = [size(arr, 1)[1] for arr in inputs]\n",
    "numCSVs = size(inputs)[1]\n",
    "SEQ_MIN = minimum(rows)\n",
    "SEQ_MAX = maximum(rows)\n",
    "\n",
    "println(\"Number of CSVs: $numCSVs \" * \"Min Rows: $SEQ_MIN \" * \"Max Rows: $SEQ_MAX \" * \"Number of columns: $(size(inputs[1])[2])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    RNNCell(500 => 128, tanh),          \u001b[90m# 80_640 parameters\u001b[39m\n",
       "  ),\n",
       "  Recur(\n",
       "    LSTMCell(128 => 64),                \u001b[90m# 49_536 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(64 => 1),                       \u001b[90m# 65 parameters\u001b[39m\n",
       "  NNlib.relu,\n",
       ") \u001b[90m        # Total: 11 trainable arrays, \u001b[39m130_241 parameters,\n",
       "\u001b[90m          # plus 3 non-trainable, 256 parameters, summarysize \u001b[39m509.457 KiB."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = Flux.Chain(\n",
    "    Flux.RNN(500, 128, tanh),\n",
    "    Flux.LSTM(128, 64),\n",
    "    Flux.Dense(64, 1),\n",
    "    Flux.relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((cell = (σ = (), Wi = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, Wh = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, b = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, state0 = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), state = ()), (cell = (Wi = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, Wh = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, b = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, state0 = (\u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m, \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m)), state = ()), (weight = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0], Float32[0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), ()),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_gradient(model, x, y)\n",
    "    loss = Flux.Losses.mse\n",
    "    loss_val = loss(model(x), y)\n",
    "    grad = Flux.gradient(() -> loss(model(x), y), Flux.params(model))\n",
    "    return loss_val, grad\n",
    "end\n",
    "\n",
    "opt = Flux.Optimise.Adam(learning_rate)\n",
    "opt = Flux.setup(opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_batches (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batches(r, xs, ys) = (Flux.batch(xs[r]), Flux.batch(ys[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_model (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_model(model, train_data, val_data, epochs, opt)\n",
    "    train_inputs, train_outputs = train_data\n",
    "    val_inputs, val_outputs = val_data\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = Inf\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        num_batches = ceil(length(train_inputs) / batch_size)\n",
    "\n",
    "        # Train the model\n",
    "        for i in 1:num_batches\n",
    "            r = Int((i-1)*batch_size+1) : Int(min(i*batch_size, length(train_inputs)))\n",
    "            println(r)\n",
    "            x, y = create_batches(r, train_inputs, train_outputs)\n",
    "            loss_val, grad = loss_gradient(model, x, y)\n",
    "            update!(opt, Flux.params(model), grad)\n",
    "            train_loss += loss_val\n",
    "        end\n",
    "\n",
    "        # Compute validation loss\n",
    "        for i in 1:ceil(length(val_inputs) / batch_size)\n",
    "            r = (i-1)*batch_size+1 : min(i*batch_size, length(val_inputs))\n",
    "            x, y = create_batches(r, val_inputs, val_outputs)\n",
    "            val_loss += Flux.mae(model(x), y)\n",
    "        end\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        val_loss /= ceil(length(val_inputs) / batch_size)\n",
    "        push!(train_losses, train_loss)\n",
    "        push!(val_losses, val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        println(\"Epoch $(epoch) - Train loss: $(train_loss), Val loss: $(val_loss)\")\n",
    "\n",
    "        # Check if the current model is the best so far\n",
    "        if val_loss < best_val_loss\n",
    "            best_val_loss = val_loss\n",
    "            best_model_params = deepcopy(Flux.params(model))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Restore the best model parameters\n",
    "    Flux.loadparams!(model, best_model_params)\n",
    "\n",
    "    # Return the trained model and the loss history\n",
    "    return model, train_losses, val_losses\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:32\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch: loss function expects size(ŷ) = (1, 4, 32) to match size(y) = (32,)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: loss function expects size(ŷ) = (1, 4, 32) to match size(y) = (32,)",
      "",
      "Stacktrace:",
      " [1] _check_sizes(ŷ::Array{Float32, 3}, y::Vector{Any})",
      "   @ Flux.Losses C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\losses\\utils.jl:31",
      " [2] mse(ŷ::Array{Float32, 3}, y::Vector{Any}; agg::typeof(mean))",
      "   @ Flux.Losses C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\losses\\functions.jl:46",
      " [3] mse",
      "   @ C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\losses\\functions.jl:45 [inlined]",
      " [4] loss_gradient(model::Flux.Chain{Tuple{Flux.Recur{Flux.RNNCell{typeof(tanh), Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}}, Matrix{Float32}}, Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu)}}, x::Array{Float32, 3}, y::Vector{Any})",
      "   @ Main .\\In[16]:3",
      " [5] train_model(model::Flux.Chain{Tuple{Flux.Recur{Flux.RNNCell{typeof(tanh), Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}}, Matrix{Float32}}, Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(relu)}}, train_data::Tuple{Vector{Any}, Vector{Any}}, val_data::Tuple{Vector{Any}, Vector{Any}}, epochs::Int64, opt::NamedTuple{(:layers,), Tuple{Tuple{NamedTuple{(:cell, :state), Tuple{NamedTuple{(:σ, :Wi, :Wh, :b, :state0), Tuple{Tuple{}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}}}, Tuple{}}}, NamedTuple{(:cell, :state), Tuple{NamedTuple{(:Wi, :Wh, :b, :state0), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float64, Float64}}}, Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}}}}, Tuple{}}}, NamedTuple{(:weight, :bias, :σ), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float64, Float64}}}, Tuple{}}}, Tuple{}}}})",
      "   @ Main .\\In[18]:18",
      " [6] top-level scope",
      "   @ In[19]:1"
     ]
    }
   ],
   "source": [
    "train_model(model, train_data, val_data, epochs, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the validation set\n",
    "val_predictions = [model(reshape(x, size(x, 1), 1, size(x, 2))) for x in X_val]\n",
    "val_loss = mean(loss_function(ŷ, y) for (ŷ, y) in zip(val_predictions, y_val))\n",
    "println(\"Validation Loss: $val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function preprocess_new_midi(csv_file::String)\n",
    "    df = CSV.read(csv_file, DataFrame)\n",
    "    if size(df)[1] < 500\n",
    "        println(\"MIDI file is too short (< 500 rows).\")\n",
    "        return nothing\n",
    "    end\n",
    "    input = Matrix{Float32}(df[1:500,Not(:anomalies)])\n",
    "    if sum(df.anomalies[1:500]) > 0\n",
    "        println(\"MIDI file contains $(sum(df.anomalies[1:500])) anomalies.\")\n",
    "    end\n",
    "    return input\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save \"initModel.bson\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BSON.@load model_filename model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new MIDI file\n",
    "new_midi_file = \"assets/anomalous/liz_rhap15_0.3.csv\"\n",
    "new_midi_input = preprocess_new_midi(new_midi_file)\n",
    "\n",
    "# Check if the preprocessing was successful (the file had at least 500 rows)\n",
    "if new_midi_input !== nothing\n",
    "    # Reshape the input array to match the model's input shape\n",
    "    new_midi_input = reshape(new_midi_input, size(new_midi_input, 1), 1, size(new_midi_input, 2))\n",
    "    println(new_midi_input)    \n",
    "    # Predict the number of errors in the new MIDI file\n",
    "    num_errors = model(new_midi_input)\n",
    "\n",
    "    println(\"Predicted number of errors: \", num_errors[1])\n",
    "else\n",
    "    println(\"Prediction cannot be performed due to insufficient data.\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
