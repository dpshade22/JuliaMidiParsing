{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Zygote\n",
    "using Flux.Data: DataLoader\n",
    "using Statistics: mean\n",
    "using JLD2\n",
    "include(\"preprocessing.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Data: DataLoader\n",
    "\n",
    "function prepare_dataloader(input_dir::String, train_ratio::Float64=0.8, batch_size::Int=32)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    for csv in readdir(input_dir)\n",
    "        df = CSV.read(joinpath(input_dir, csv), DataFrame)\n",
    "        if size(df)[1] < 500\n",
    "            continue\n",
    "        end\n",
    "        push!(inputs, Matrix{Float32}(df[1:500, 1:end-1])')\n",
    "        push!(outputs, sum(df[1:500, end]))\n",
    "    end\n",
    "    \n",
    "    # Find the length of the longest input array\n",
    "    max_length = maximum(size(input, 1) for input in inputs)\n",
    "\n",
    "    # Pad input arrays with zeros to match the longest array's length\n",
    "    padded_inputs = []\n",
    "    for input in inputs\n",
    "        rows_to_pad = max_length - size(input, 1)\n",
    "        padded_input = vcat(input, zeros(Float32, rows_to_pad, size(input, 2)))\n",
    "        push!(padded_inputs, padded_input)\n",
    "    end\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    num_train = Int(round(length(padded_inputs) * train_ratio))\n",
    "    train_inputs = padded_inputs[1:num_train]\n",
    "    train_outputs = outputs[1:num_train]\n",
    "    val_inputs = padded_inputs[num_train+1:end]\n",
    "    val_outputs = outputs[num_train+1:end]\n",
    "\n",
    "    train_data = Flux.Data.DataLoader((train_inputs, train_outputs), batchsize=batch_size, shuffle=true)\n",
    "    val_data = Flux.Data.DataLoader((val_inputs, val_outputs), batchsize=batch_size, shuffle=true)\n",
    "\n",
    "    return train_data, val_data\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = prepare_data(\"assets/anomalous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random: shuffle\n",
    "using DataFrames: nrow\n",
    "\n",
    "function split_data(inputs, outputs, train_frac=0.8)\n",
    "    data = collect(zip(inputs, outputs))\n",
    "    data = shuffle(data)\n",
    "    println(size(data))\n",
    "\n",
    "    train_size = Int(floor(train_frac * size(data)[1]))\n",
    "    train_data = data[1:train_size]\n",
    "    val_data = data[train_size+1:end]\n",
    "    \n",
    "    return train_data, val_data\n",
    "end\n",
    "\n",
    "train_data, val_data = split_data(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many-to-one RNN architecture\n",
    "model = Flux.Chain(\n",
    "    x -> reshape(x, 4, 1, 500),\n",
    "    Flux.LSTM(4, 64),\n",
    "    x -> x[:, :, end],  # Select the hidden state at the last time step\n",
    "    Dense(64, 1)\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.mse(model(x), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Flux.setup(Adam(1e-2), model)\n",
    "ps = Flux.params(model) # Get the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_loader = DataLoader(train_data, batchsize=32, shuffle=true);\n",
    "println(train_loader)\n",
    "\n",
    "for epoch in 1:num_epochs\n",
    "    for (x, y) in train_loader\n",
    "        val, grads = Flux.withgradient(model) do m\n",
    "            ŷ = m(x)\n",
    "            loss_val = Flux.Losses.mse(ŷ, y)\n",
    "        end\n",
    "\n",
    "        if !isfinite(val)\n",
    "            @warn \"loss is $val on item $epoch\" epoch\n",
    "            continue\n",
    "        end\n",
    "        if length(grads) > 0\n",
    "            Flux.Optimise.update!(opt, ps, grads[1])\n",
    "        else\n",
    "            @warn \"no gradients on item $epoch\" epoch\n",
    "        end\n",
    "    end\n",
    "    # Optionally, you can evaluate the model on the validation set and print the validation loss here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the validation set\n",
    "val_predictions = [model(reshape(x, size(x, 1), 1, size(x, 2))) for x in X_val]\n",
    "val_loss = mean(loss_function(ŷ, y) for (ŷ, y) in zip(val_predictions, y_val))\n",
    "println(\"Validation Loss: $val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function preprocess_new_midi(csv_file::String)\n",
    "    df = CSV.read(csv_file, DataFrame)\n",
    "    if size(df)[1] < 500\n",
    "        println(\"MIDI file is too short (< 500 rows).\")\n",
    "        return nothing\n",
    "    end\n",
    "    input = Matrix{Float32}(df[1:500,Not(:anomalies)])\n",
    "    if sum(df.anomalies[1:500]) > 0\n",
    "        println(\"MIDI file contains $(sum(df.anomalies[1:500])) anomalies.\")\n",
    "    end\n",
    "    return input\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new MIDI file\n",
    "new_midi_file = \"assets/anomalous/liz_rhap15_0.3.csv\"\n",
    "new_midi_input = preprocess_new_midi(new_midi_file)\n",
    "\n",
    "# Check if the preprocessing was successful (the file had at least 500 rows)\n",
    "if new_midi_input !== nothing\n",
    "    # Reshape the input array to match the model's input shape\n",
    "    new_midi_input = reshape(new_midi_input, size(new_midi_input, 1), 1, size(new_midi_input, 2))\n",
    "    println(new_midi_input)    \n",
    "    # Predict the number of errors in the new MIDI file\n",
    "    num_errors = model(new_midi_input)\n",
    "\n",
    "    println(\"Predicted number of errors: \", num_errors[1])\n",
    "else\n",
    "    println(\"Prediction cannot be performed due to insufficient data.\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
