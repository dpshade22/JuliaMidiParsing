{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reconstruct_midi_file (generic function with 1 method)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux, Zygote\n",
    "using Flux.Data: DataLoader\n",
    "using Statistics: mean\n",
    "using JLD2\n",
    "include(\"preprocessing.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_dataloader (generic function with 3 methods)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_dataloader(input_dir::String, train_ratio::Float64=0.8, batch_size::Int=32)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    println(\"Reading data from $(input_dir)...\")\n",
    "    for csv in readdir(input_dir)\n",
    "        df = CSV.read(joinpath(input_dir, csv), DataFrame)\n",
    "        if size(df)[1] < 500\n",
    "            continue\n",
    "        end\n",
    "        push!(inputs, Matrix{Float32}(df[1:500, 1:end-1])')\n",
    "        push!(outputs, sum(df[1:500, end]))\n",
    "    end\n",
    "    \n",
    "    # Find the length of the longest input array\n",
    "    max_length = maximum(size(input, 1) for input in inputs)\n",
    "    println(\"Padding inputs to length $(max_length)...\")\n",
    "    # Pad input arrays with zeros to match the longest array's length\n",
    "    padded_inputs = []\n",
    "    for input in inputs\n",
    "        rows_to_pad = max_length - size(input, 1)\n",
    "        padded_input = vcat(input, zeros(Float32, rows_to_pad, size(input, 2)))\n",
    "        push!(padded_inputs, padded_input)\n",
    "    end\n",
    "\n",
    "    println(\"Batching data...\")\n",
    "    # Split the data into train and validation sets\n",
    "    num_train = Int(round(length(padded_inputs) * train_ratio))\n",
    "    train_inputs = padded_inputs[1:num_train]\n",
    "    train_outputs = outputs[1:num_train]\n",
    "    val_inputs = padded_inputs[num_train+1:end]\n",
    "    val_outputs = outputs[num_train+1:end]\n",
    "\n",
    "    train_data = Flux.Data.DataLoader((train_inputs, train_outputs), batchsize=batch_size, shuffle=true, partial=false)\n",
    "    val_data = Flux.Data.DataLoader((val_inputs, val_outputs), batchsize=batch_size, shuffle=true, partial=false)\n",
    "    println(\"Done preparing data.\")\n",
    "\n",
    "    return train_data, val_data\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from assets/anomalous...\n",
      "Padding inputs to length 4...\n",
      "Batching data...\n",
      "Done preparing data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataLoader(::Tuple{Vector{Any}, Vector{Any}}, shuffle=true, batchsize=32, partial=false), DataLoader(::Tuple{Vector{Any}, Vector{Any}}, shuffle=true, batchsize=32, partial=false))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set, val_set = prepare_dataloader(\"assets/anomalous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many-to-one RNN architecture\n",
    "model = Flux.Chain(\n",
    "    Flux.LSTM(4, 64),\n",
    "    x -> x[:, :, end],  # Select the hidden state at the last time step\n",
    "    Dense(64, 1)\n",
    ")\n",
    "\n",
    "loss(x, y) = Flux.mse(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.097776175 0.124119595 -0.13322908 -0.019199725; -0.051209584 0.08477816 -0.10397716 0.049686965; … ; -0.038245685 0.016786586 0.071300104 -0.10400492; -0.098428905 0.0021999406 -0.022911513 0.031021858], Float32[0.04454969 0.09986814 … 0.06935859 0.054235328; -0.12886257 -0.11384021 … 0.068782404 0.045261636; … ; -0.072473325 0.1240657 … -0.07712732 0.11565524; 0.033414874 0.053624082 … -0.05203304 -0.12691651], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.17766395 0.10723281 … 0.23744194 0.24607086], Float32[0.0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Flux.setup(Adam(1e-2), model)\n",
    "ps = Flux.params(model) # Get the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in 1:num_epochs\n",
    "    for (x, y) in train_set\n",
    "        lstm_input = cat(x..., dims=3)  # Concatenate along the third dimension\n",
    "        lstm_input = permutedims(lstm_input, (1, 3, 2))  # Transpose dimensions to (sequence_length, batch_size, n_features)\n",
    "        y = reshape(y, (length(y), 1)) # Reshape y to match model output shape\n",
    "\n",
    "        val, grads = Flux.withgradient(model) do m\n",
    "            ŷ = m(lstm_input)\n",
    "            ŷ = reshape(ŷ, (size(ŷ, 2), 1))\n",
    "            loss_val = Flux.Losses.mse(ŷ, y)\n",
    "        end\n",
    "    end\n",
    "    # Optionally, you can evaluate the model on the validation set and print the validation loss here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_model (generic function with 1 method)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_model(model, val_set)\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for (x, y) in val_set\n",
    "        # Preprocess input data\n",
    "        lstm_input = cat(x..., dims=3)  # Concatenate along the third dimension\n",
    "        lstm_input = permutedims(lstm_input, (1, 3, 2))  # Transpose dimensions to (sequence_length, batch_size, n_features)\n",
    "\n",
    "        # Expand the dimensions of the target data\n",
    "        y = reshape(y, (length(y), 1)) # Reshape y to match model output shape\n",
    "\n",
    "        # Compute predictions and loss\n",
    "        ŷ = model(lstm_input)\n",
    "        ŷ = reshape(ŷ, (size(ŷ, 2), 1))\n",
    "        batch_loss = Flux.Losses.mse(ŷ, y)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        num_batches += 1\n",
    "    end\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save \"trained_model.jld2\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: m not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: m not defined",
      "",
      "Stacktrace:",
      " [1] evaluate_model(model::Flux.Chain{Tuple{Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}, var\"#109#110\", Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, val_set::DataLoader{Tuple{Vector{Any}, Vector{Any}}, Random._GLOBAL_RNG, Val{nothing}})",
      "   @ Main .\\In[128]:14",
      " [2] top-level scope",
      "   @ In[129]:1"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "KeyError: key \"loaded_model\" not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key \"loaded_model\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(g::JLD2.Group{JLD2.JLDFile{JLD2.MmapIO}}, name::String)",
      "   @ JLD2 C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\groups.jl:101",
      " [2] read",
      "   @ C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\JLD2.jl:457 [inlined]",
      " [3] (::var\"#131#132\")(f::JLD2.JLDFile{JLD2.MmapIO})",
      "   @ Main C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\loadsave.jl:146",
      " [4] jldopen(f::Function, args::String; kws::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "   @ JLD2 C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\loadsave.jl:4",
      " [5] jldopen(f::Function, args::String)",
      "   @ JLD2 C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\loadsave.jl:1",
      " [6] top-level scope",
      "   @ C:\\Users\\dpsha\\.julia\\packages\\JLD2\\ryhNR\\src\\loadsave.jl:145"
     ]
    }
   ],
   "source": [
    "@load \"trained_model.jld2\" loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess_new_midi (generic function with 1 method)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function preprocess_new_midi(csv_file::String)\n",
    "    df = CSV.read(csv_file, DataFrame)\n",
    "    if size(df)[1] < 500\n",
    "        println(\"MIDI file is too short (< 500 rows).\")\n",
    "        return nothing\n",
    "    end\n",
    "    input = Matrix{Float32}(df[1:500, Not(:anomalies)])\n",
    "    if sum(df.anomalies[1:500]) > 0\n",
    "        println(\"MIDI file contains $(sum(df.anomalies[1:500])) anomalies.\")\n",
    "    end\n",
    "    println((size(input, 1), 1, size(input, 2)))\n",
    "\n",
    "    # Transpose the input data and reshape it for LSTM\n",
    "    lstm_input = transpose(input)\n",
    "    lstm_input = reshape(lstm_input, (size(lstm_input, 1), size(lstm_input, 2), 1))\n",
    "    \n",
    "    return lstm_input\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file contains 82 anomalies.\n",
      "(500, 1, 4)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 500 and 32",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 500 and 32",
      "",
      "Stacktrace:",
      "  [1] _bcs1",
      "    @ .\\broadcast.jl:516 [inlined]",
      "  [2] _bcs (repeats 2 times)",
      "    @ .\\broadcast.jl:510 [inlined]",
      "  [3] broadcast_shape",
      "    @ .\\broadcast.jl:504 [inlined]",
      "  [4] combine_axes",
      "    @ .\\broadcast.jl:499 [inlined]",
      "  [5] instantiate",
      "    @ .\\broadcast.jl:281 [inlined]",
      "  [6] materialize",
      "    @ .\\broadcast.jl:860 [inlined]",
      "  [7] muladd(A::Matrix{Float32}, y::SubArray{Float32, 2, Base.ReshapedArray{Float32, 3, LinearAlgebra.Transpose{Float32, Matrix{Float32}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}, Int64}, false}, z::Matrix{Float32})",
      "    @ LinearAlgebra C:\\Users\\dpsha\\AppData\\Local\\Programs\\Julia-1.8.5\\share\\julia\\stdlib\\v1.8\\LinearAlgebra\\src\\matmul.jl:218",
      "  [8] (::Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}})(::Tuple{Matrix{Float32}, Matrix{Float32}}, x::SubArray{Float32, 2, Base.ReshapedArray{Float32, 3, LinearAlgebra.Transpose{Float32, Matrix{Float32}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}, Int64}, false})",
      "    @ Flux C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\recurrent.jl:314",
      "  [9] Recur",
      "    @ C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\recurrent.jl:134 [inlined]",
      " [10] #288",
      "    @ .\\none:0 [inlined]",
      " [11] iterate",
      "    @ .\\generator.jl:47 [inlined]",
      " [12] collect(itr::Base.Generator{Base.Generator{Base.OneTo{Int64}, Flux.var\"#278#280\"{Base.ReshapedArray{Float32, 3, LinearAlgebra.Transpose{Float32, Matrix{Float32}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}, Tuple{Colon, Colon}}}, Flux.var\"#288#289\"{Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}}})",
      "    @ Base .\\array.jl:787",
      " [13] Recur",
      "    @ C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\recurrent.jl:185 [inlined]",
      " [14] macro expansion",
      "    @ C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\basic.jl:53 [inlined]",
      " [15] _applychain(layers::Tuple{Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}, var\"#109#110\", Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}, x::Base.ReshapedArray{Float32, 3, LinearAlgebra.Transpose{Float32, Matrix{Float32}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}})",
      "    @ Flux C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\basic.jl:53",
      " [16] (::Flux.Chain{Tuple{Flux.Recur{Flux.LSTMCell{Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Tuple{Matrix{Float32}, Matrix{Float32}}}, Tuple{Matrix{Float32}, Matrix{Float32}}}, var\"#109#110\", Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}})(x::Base.ReshapedArray{Float32, 3, LinearAlgebra.Transpose{Float32, Matrix{Float32}}, Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}})",
      "    @ Flux C:\\Users\\dpsha\\.julia\\packages\\Flux\\Nzh8J\\src\\layers\\basic.jl:51",
      " [17] top-level scope",
      "    @ In[121]:3"
     ]
    }
   ],
   "source": [
    "preprocessed_input = preprocess_new_midi(\"assets/anomalous/[ajin_op]_yoru_wa_nemureru_kai_-__flumpool__fonzi_m__0.1.csv\")\n",
    "if preprocessed_input !== nothing\n",
    "    ŷ = model(preprocessed_input)\n",
    "    # Process the output ŷ as needed\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new MIDI file\n",
    "new_midi_file = \"assets/anomalous/liz_rhap15_0.3.csv\"\n",
    "new_midi_input = preprocess_new_midi(new_midi_file)\n",
    "\n",
    "# Check if the preprocessing was successful (the file had at least 500 rows)\n",
    "if new_midi_input !== nothing\n",
    "    # Reshape the input array to match the model's input shape\n",
    "    new_midi_input = reshape(new_midi_input, size(new_midi_input, 1), 1, size(new_midi_input, 2))\n",
    "    println(new_midi_input)    \n",
    "    # Predict the number of errors in the new MIDI file\n",
    "    num_errors = model(new_midi_input)\n",
    "\n",
    "    println(\"Predicted number of errors: \", num_errors[1])\n",
    "else\n",
    "    println(\"Prediction cannot be performed due to insufficient data.\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
